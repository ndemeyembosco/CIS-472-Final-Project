{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "ind_file = open('FilmMissingIndices.txt', 'r')\n",
    "with open('FilmMissingIndicesTrueValueUnicode.txt', newline='', encoding='utf16') as f:\n",
    "    t_val_file = list(map(lambda x: float(x.strip()), f.readlines()))\n",
    "mat = scipy.io.loadmat('FilmPositiveNegative.mat')\n",
    "missing_indices = np.array([[int(s.split()[0]), int(s.split()[1])] for s in ind_file.readlines()])\n",
    "data = mat['FilmPositiveNegative']\n",
    "len(t_val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1350000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 50)\n",
      "(1350000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(missing_indices.shape)\n",
    "\n",
    "(users, films) = data.shape\n",
    "(height, width) = (users, films)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "sdata = preprocessing.scale(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 50, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import ndarray as nd \n",
    "new_l = []\n",
    "for i in range(1, height+1):\n",
    "    new_li = []\n",
    "    for j in range(1, width+1):\n",
    "        first = data[i-1][j-1] if data[i-1][j-1] > 0 else -data[i-1][j-1]\n",
    "        snd   = 1 if data[i-1][j-1] > 0 else 0\n",
    "        new_li.append((i, first, snd))\n",
    "    new_l.append(new_li)\n",
    "new_arr = np.array(new_l)  \n",
    "print(new_arr.shape)\n",
    "nnew_arr = nd.flatten(new_arr)\n",
    "# nnew_arr.shape\n",
    "ndata = nnew_arr.reshape(150000,3)\n",
    "ndata.shape\n",
    "# ndata=nnew_arr \n",
    "# ndata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have 3000 users and 50 films. We need to divide up this into training set, development set and test set. \n",
    "      * training set (70%) -- (2100,50)\n",
    "      * development set (20%) -- (600, 50)\n",
    "      * testing set (10%) -- (300, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train = ndata[:105000]\n",
    "m_dev   = ndata[105000:135000]\n",
    "m_test  = ndata[135000:150000]\n",
    "\n",
    "ind_train  = missing_indices[:105000]\n",
    "tval_train = t_val_file[:105000]\n",
    "\n",
    "ind_dev   = missing_indices[105000:135000]\n",
    "tval_dev  = t_val_file[105000:135000]\n",
    "\n",
    "\n",
    "ind_test  = missing_indices[135000:150000]\n",
    "tval_test = t_val_file[135000:150000]\n",
    "\n",
    "ind_unseen   = missing_indices[150000:]\n",
    "tval_unseen = t_val_file[150000:]\n",
    "\n",
    "(h, w)= m_train.shape\n",
    "m_train[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider three models\n",
    "  * K-nearest neighbor\n",
    "  * The perceptron\n",
    "  * Gradient descent with squared loss\n",
    "For each of these models, we will choose a list of settings of hyperparameters. \n",
    "\n",
    "For each setting in the list, we will train our model\n",
    "using that setting. We will compute error rate on development data, and after all settings are done,\n",
    "\n",
    "We will choose one with smallest error rate on development data. We will then evaluate this model on test data to estimate future performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only hyperparameter to tune in this case is the K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "hyper_params = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# calculate all the models for each param setting\n",
    "models = []\n",
    "for k in hyper_params:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='ball_tree')\n",
    "    models.append(clf)\n",
    "\n",
    "# train each model on training data\n",
    "for m in models:\n",
    "    m.fit(m_train[:, :2], m_train[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that if we do predict_proba, we can choose the last element in the list of results, which contains [p, 1- p] for each example, and so, we can just choose the first or second element of the list. This can then be the probability I calculate the squared error for, for each model and choose one with the least error. Another alternative would be to use the score function from sklearn, and choose the model that scores highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5421893888055477"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "main_model = [models[1], 0]\n",
    "for m in models:\n",
    "    main_model[1] = main_model[0].score(m_dev[:, :2], m_dev[:, 2])\n",
    "    y_pred = m.predict_proba(m_dev[:, :2])[:, 1]\n",
    "    y_true = m_dev[:, 2]\n",
    "    s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    scores.append(s)\n",
    "    if s > main_model[1]:\n",
    "        main_model = [m, s]\n",
    "    \n",
    "scores \n",
    "main_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5425673537297774"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model = main_model[0]\n",
    "y_pred = m.predict_proba(m_test[:, :2])[:, 1]\n",
    "y_true = m_test[:, 2]\n",
    "s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter in this case is the number of iterations until convergence. Moreover, we should shuffle per iteration of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we should probably consider two different version: regularized vs non-regularized perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe also consider not shuffling and see how that impacts its behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, early stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronHParam:\n",
    "    def __init__(self, iterations, regular, shuffled, estop):\n",
    "        self.iter     = iterations\n",
    "        self.reg      = regular \n",
    "        self.shuffled = shuffled\n",
    "        self.estop    = estop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_1000 = [PerceptronHParam(1000, True, True, True)\n",
    "                   , PerceptronHParam(1000, False, True, True)\n",
    "                   , PerceptronHParam(1000, False, True, False)\n",
    "                   , PerceptronHParam(1000, False, False, False)\n",
    "                   , PerceptronHParam(1000, True, False, True)\n",
    "                   , PerceptronHParam(1000, True, False, False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron \n",
    "def make_model(p, reg=None, alpha=None):\n",
    "    clf = None\n",
    "    if p.reg:\n",
    "        if p.shuffled:\n",
    "            if p.estop:\n",
    "                clf = Perceptron(max_iter=p.iter, penalty=reg, alpha=alpha, early_stopping=True)\n",
    "            else:\n",
    "                clf = Perceptron(max_iter=p.iter, penalty=reg, alpha=alpha)\n",
    "        else:\n",
    "            clf = Perceptron(max_iter=p.iter, penalty=reg, alpha=alpha, early_stopping=p.estop, shuffle=False)\n",
    "    else:\n",
    "        clf = Perceptron(max_iter=p.iter, early_stopping=p.estop, shuffle=p.shuffled)\n",
    "    return clf    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, given a set of hyperparameters, make_model will construct a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_iters = [1000, 1500, 2000]\n",
    "def make_hparams_per_iter(iters):\n",
    "    return [PerceptronHParam(iters, True, True, True)\n",
    "                   , PerceptronHParam(iters, False, True, True)\n",
    "                   , PerceptronHParam(iters, False, True, False)\n",
    "                   , PerceptronHParam(iters, False, False, False)\n",
    "                   , PerceptronHParam(iters, True, False, True)\n",
    "                   , PerceptronHParam(iters, True, False, False)]\n",
    "hparam_objs = [make_hparams_per_iter(it) for it in h_iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "regs   = [None, 'l2','l1','elasticnet']\n",
    "alphas = [0.0001, 0.001, 0.01]\n",
    "for ob_l in hparam_objs:\n",
    "    for p in ob_l: \n",
    "        for r in regs:\n",
    "            for a in alphas:\n",
    "                classifier = make_model(p, r, a)\n",
    "                models.append(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up our hyperparameters, we can proceed to train, develop and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    m.fit(m_train[:, :2], m_train[:, 2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268636762326032"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "main_model = [models[1], 0]\n",
    "for m in models:\n",
    "    main_model[1] = main_model[0].score(m_dev[:, :2], m_dev[:, 2])\n",
    "    y_pred = m.predict_proba(m_dev[:, :2])[:, 1]\n",
    "    y_true = m_dev[:, 2]\n",
    "    s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    scores.append(s)\n",
    "    if s > main_model[1]:\n",
    "        main_model = [m, s]\n",
    "    \n",
    "scores \n",
    "main_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5279659079902792"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model = main_model[0]\n",
    "y_pred = m.predict_proba(m_test[:, :2])[:, 1]\n",
    "y_true = m_test[:, 2]\n",
    "s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to tune include the regularizer weight, and gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegHParam:\n",
    "    def __init__(self, f_interp=True, reg=None, reg_weight=0):\n",
    "        self.f_int          = f_interp\n",
    "        self.regularizer    = reg \n",
    "        self.regular_weight = reg_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalties = ['l1', 'l2', 'elasticnet']\n",
    "interp = [True, False]\n",
    "cs = [0.0001, 0.001, 0.01, 1]\n",
    "h_params = []\n",
    "for p in penalties:\n",
    "    for c in cs:\n",
    "        for i in interp:\n",
    "            ob = LogRegHParam(i, p, c)\n",
    "            h_params.append(ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(penalty=p.regularizer, fit_intercept=p.f_int, C=p.regular_weight,  solver='saga', max_iter=1000) for p in h_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(penalty='l2', fit_intercept=p.f_int, C=p.regular_weight) for p in h_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    m.fit(m_train[:, :2], m_train[:, 2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268636762326032"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "main_model = [models[1], 0]\n",
    "for m in models:\n",
    "    main_model[1] = main_model[0].score(m_dev[:, :2], m_dev[:, 2])\n",
    "    y_pred = m.predict_proba(m_dev[:, :2])[:, 1]\n",
    "    y_true = m_dev[:, 2]\n",
    "    s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    scores.append(s)\n",
    "    if s > main_model[1]:\n",
    "        main_model = [m, s]\n",
    "    \n",
    "scores \n",
    "main_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5279659079902792"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model = main_model[0]\n",
    "y_pred = m.predict_proba(m_test[:, :2])[:, 1]\n",
    "y_true = m_test[:, 2]\n",
    "s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_split = [2, 4, 8]\n",
    "models = []\n",
    "for d in depths_split:\n",
    "    clf = tree.DecisionTreeClassifier(min_samples_split=d)\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    m.fit(m_train[:, :2], m_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268636762326032"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "main_model = [models[1], 0]\n",
    "for m in models:\n",
    "    main_model[1] = main_model[0].score(m_dev[:, :2], m_dev[:, 2])\n",
    "    y_pred = m.predict_proba(m_dev[:, :2])[:, 1]\n",
    "    y_true = m_dev[:, 2]\n",
    "    s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    scores.append(s)\n",
    "    if s > main_model[1]:\n",
    "        main_model = [m, s]\n",
    "    \n",
    "scores \n",
    "main_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5279659079902792"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model = main_model[0]\n",
    "y_pred = m.predict_proba(m_test[:, :2])[:, 1]\n",
    "y_true = m_test[:, 2]\n",
    "s = mean_squared_error(y_true, y_pred, squared=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "clf = svm.SVC()\n",
    "clf.fit(m_train[:, :2], m_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5444"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(m_test[:, :2], m_test[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-3ef51afae2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlinear_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "linear_clf = svm.LinearSVC()\n",
    "clf.fit(m_train[:, :2], m_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(m_test[:, :2], m_test[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(m_train[:, :2], m_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
